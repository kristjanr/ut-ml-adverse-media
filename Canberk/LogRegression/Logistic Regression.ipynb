{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative News Neural Nets Project: Classifying Adverse Media Articles using Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, conda environment with Python 3.86 is used. Some libraries, such as spacy and nltk may require installation if your machine does not have them. \n",
    "\n",
    "You can use the steps below to install spaCy. If something goes awry, feel free to use pip/do some stackoverflow search to complete the installation. The last two parts will be required later on in the notebook, they are not essential spaCy packages.\n",
    "\n",
    " - conda install -c conda-forge spacy\n",
    " \n",
    " - conda install -c conda-forge spacy-lookups-data\n",
    " \n",
    " - python -m spacy download en_core_web_sm\n",
    " \n",
    " - pip install spacy-langdetect\n",
    " \n",
    " - conda install -c conda-forge wordcloud\n",
    " \n",
    "On the other hand, installing nltk packages will be easy, just look at the error to understand what needs to be downloaded using nltk.download(...). I have already provided the download code for punkt package and I don't think anything is required beside that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF & Baseline Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a short notebook, reserved only for the Logistic Regression Model. We will use the cleaned & lemmatized dataset that we have exported as a .csv file during the preprocessing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score, f1_score, confusion_matrix\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# This module will be for saving the trained model for later use\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you're using linux\n",
    "# !ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Win 10\n",
      " Volume Serial Number is CA9A-F06E\n",
      "\n",
      " Directory of C:\\Users\\canberk\\Desktop\\ut-ml-adverse-media-main\n",
      "\n",
      "12/08/2020  07:12 AM    <DIR>          .\n",
      "12/08/2020  07:12 AM    <DIR>          ..\n",
      "11/23/2020  06:08 PM           110,455 .ipynb\n",
      "12/08/2020  06:59 AM    <DIR>          .ipynb_checkpoints\n",
      "11/21/2020  04:43 PM         3,752,073 adverse_media_training.csv.zip\n",
      "12/08/2020  06:39 AM         2,174,223 cleaned_lemmatized_text.csv\n",
      "12/08/2020  06:07 AM           110,996 Data Preprocessing&Baselines-Original.ipynb\n",
      "12/08/2020  06:06 AM           115,641 Data Preprocessing&Baselines.ipynb\n",
      "11/21/2020  04:43 PM         3,630,748 EDA - Kristjan's Original.ipynb\n",
      "11/23/2020  06:04 PM         3,740,422 EDA.ipynb\n",
      "10/24/2015  07:35 PM     5,646,236,541 glove.840B.300d.txt\n",
      "12/07/2020  04:36 PM     2,176,768,927 glove.840B.300d.zip\n",
      "11/21/2020  04:43 PM             1,073 LICENSE\n",
      "12/08/2020  07:12 AM            66,944 Logistic Regression.ipynb\n",
      "12/08/2020  06:59 AM            68,615 Naive Bayes.ipynb\n",
      "11/21/2020  04:43 PM         3,764,231 non_adverse_media_training.csv.zip\n",
      "11/21/2020  04:43 PM                21 README.md\n",
      "              14 File(s)  7,840,540,910 bytes\n",
      "               3 Dir(s)  10,628,575,232 bytes free\n"
     ]
    }
   ],
   "source": [
    "# Let's get an overview of what our folder contains..\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_adverse_media</th>\n",
       "      <th>lemmatized_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>zimbabweans wake news agriculture minister per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>singapore founder singapore oil trade company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>fraudster offer green tax efficient investment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>buenos aire reuter judicial probe possible cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ukraines constitutional court appear strike bl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_adverse_media                                lemmatized_articles\n",
       "0                 0  zimbabweans wake news agriculture minister per...\n",
       "1                 1  singapore founder singapore oil trade company ...\n",
       "2                 1  fraudster offer green tax efficient investment...\n",
       "3                 1  buenos aire reuter judicial probe possible cor...\n",
       "4                 0  ukraines constitutional court appear strike bl..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./cleaned_lemmatized_text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(656,) (73,) (656,) (73,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(df['lemmatized_articles'], \n",
    "                                                    df['is_adverse_media'], \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df['is_adverse_media'])\n",
    "\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26     late september joe tone young editor dallas ob...\n",
       "257    mexicos attorney general alejandro gertz maner...\n",
       "0      zimbabweans wake news agriculture minister per...\n",
       "279    article write yash singhal vivekananda institu...\n",
       "108    singapore reuters singapores central bank impo...\n",
       "Name: lemmatized_articles, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and validation sets are ready for applying a vectorizer function. Instead of creating the document-term matrix by simply counting the number of occurrences of words(ie bag of words approach), I will apply a tf-idf vectorizer on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_df=0.5, max_features=40000, min_df=5, ngram_range=(1, 3),\n",
      "                stop_words='english')\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = TfidfVectorizer(max_features=40000,\n",
    "                             min_df=5, \n",
    "                             max_df=0.5, \n",
    "                             analyzer='word', \n",
    "                             stop_words='english', \n",
    "                             ngram_range=(1, 3))\n",
    "print(ngram_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the vectorizer to x_train and take a look at the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['harry',\n",
       " 'case include',\n",
       " 'dire',\n",
       " 'make',\n",
       " 'attorney southern',\n",
       " 'construct',\n",
       " 'americans',\n",
       " 'aml',\n",
       " 'epidemic',\n",
       " 'pump',\n",
       " 'early week',\n",
       " 'reach million',\n",
       " 'westminster magistrates',\n",
       " 'food drug',\n",
       " 'pakistani',\n",
       " 'weekly',\n",
       " 'testing available',\n",
       " 'dub',\n",
       " 'hamas',\n",
       " 'vague']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "ngram_vectorizer.fit(x_train)\n",
    "features = ngram_vectorizer.get_feature_names()\n",
    "\n",
    "random.sample(features, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfidf_train = ngram_vectorizer.transform(x_train)\n",
    "tfidf_validation = ngram_vectorizer.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.1394311 , 0.07193199,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_array = tfidf_train.toarray()\n",
    "doc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abdul</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able use</th>\n",
       "      <th>abolish</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>...</th>\n",
       "      <th>zanupf</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zetas</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimbabwe anticorruption</th>\n",
       "      <th>zimbabwe anticorruption commission</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zimbabwes</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139431</td>\n",
       "      <td>0.071932</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 6730 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandon  abdul  abdullah  abide  ability      able  able use  abolish  \\\n",
       "0      0.0    0.0       0.0    0.0      0.0  0.000000       0.0      0.0   \n",
       "1      0.0    0.0       0.0    0.0      0.0  0.000000       0.0      0.0   \n",
       "2      0.0    0.0       0.0    0.0      0.0  0.000000       0.0      0.0   \n",
       "3      0.0    0.0       0.0    0.0      0.0  0.000000       0.0      0.0   \n",
       "4      0.0    0.0       0.0    0.0      0.0  0.000000       0.0      0.0   \n",
       "5      0.0    0.0       0.0    0.0      0.0  0.000000       0.0      0.0   \n",
       "6      0.0    0.0       0.0    0.0      0.0  0.000000       0.0      0.0   \n",
       "7      0.0    0.0       0.0    0.0      0.0  0.000000       0.0      0.0   \n",
       "8      0.0    0.0       0.0    0.0      0.0  0.014276       0.0      0.0   \n",
       "9      0.0    0.0       0.0    0.0      0.0  0.000000       0.0      0.0   \n",
       "\n",
       "     abroad   absence  ...  zanupf  zealand  zero  zetas  zimbabwe  \\\n",
       "0  0.000000  0.000000  ...     0.0      0.0   0.0    0.0  0.000000   \n",
       "1  0.074467  0.000000  ...     0.0      0.0   0.0    0.0  0.000000   \n",
       "2  0.000000  0.000000  ...     0.0      0.0   0.0    0.0  0.129174   \n",
       "3  0.000000  0.016777  ...     0.0      0.0   0.0    0.0  0.000000   \n",
       "4  0.000000  0.000000  ...     0.0      0.0   0.0    0.0  0.000000   \n",
       "5  0.000000  0.000000  ...     0.0      0.0   0.0    0.0  0.000000   \n",
       "6  0.000000  0.000000  ...     0.0      0.0   0.0    0.0  0.000000   \n",
       "7  0.000000  0.000000  ...     0.0      0.0   0.0    0.0  0.000000   \n",
       "8  0.000000  0.000000  ...     0.0      0.0   0.0    0.0  0.000000   \n",
       "9  0.000000  0.000000  ...     0.0      0.0   0.0    0.0  0.000000   \n",
       "\n",
       "   zimbabwe anticorruption  zimbabwe anticorruption commission  zimbabwean  \\\n",
       "0                      0.0                                 0.0    0.000000   \n",
       "1                      0.0                                 0.0    0.000000   \n",
       "2                      0.0                                 0.0    0.139431   \n",
       "3                      0.0                                 0.0    0.000000   \n",
       "4                      0.0                                 0.0    0.000000   \n",
       "5                      0.0                                 0.0    0.000000   \n",
       "6                      0.0                                 0.0    0.000000   \n",
       "7                      0.0                                 0.0    0.000000   \n",
       "8                      0.0                                 0.0    0.000000   \n",
       "9                      0.0                                 0.0    0.000000   \n",
       "\n",
       "   zimbabwes  zone  \n",
       "0   0.000000   0.0  \n",
       "1   0.000000   0.0  \n",
       "2   0.071932   0.0  \n",
       "3   0.000000   0.0  \n",
       "4   0.000000   0.0  \n",
       "5   0.000000   0.0  \n",
       "6   0.000000   0.0  \n",
       "7   0.000000   0.0  \n",
       "8   0.000000   0.0  \n",
       "9   0.000000   0.0  \n",
       "\n",
       "[10 rows x 6730 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_matrix = pd.DataFrame(doc_array, \n",
    "                                columns = features)\n",
    "frequency_matrix.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the train and the validation datasets are transformed. Now we need to fit a  basic logistic regression model to see how far it can get with f1 score and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='sag')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='sag')\n",
    "lr.fit(tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_lr = lr.predict(tfidf_train)\n",
    "val_preds_lr = lr.predict(tfidf_validation)\n",
    "\n",
    "train_f1_score_lr = f1_score(y_train, train_preds_lr)\n",
    "val_f1_score_lr = f1_score(y_val, val_preds_lr)\n",
    "\n",
    "train_accuracy_lr = accuracy_score(y_train, train_preds_lr)\n",
    "val_accuracy_lr = accuracy_score(y_val, val_preds_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy for logistic regression model on train data: 96.799\n",
      "Prediction accuracy for logistic regression model on validation data: 87.671\n",
      "\n",
      "F1 score for logistic regression model on train data: 97.219\n",
      "F1 score for logistic regression model on validation data: 89.888\n"
     ]
    }
   ],
   "source": [
    "print('Prediction accuracy for logistic regression model on train data:', round(train_accuracy_lr*100, 3))\n",
    "print('Prediction accuracy for logistic regression model on validation data:', round(val_accuracy_lr*100, 3))\n",
    "\n",
    "print()\n",
    "\n",
    "print('F1 score for logistic regression model on train data:', round(train_f1_score_lr*100, 3))\n",
    "print('F1 score for logistic regression model on validation data:', round(val_f1_score_lr*100, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seem good, but they can get better, in test data we will most probably see some overfitting. **Yet, for now, I am skipping the regularization part, since I would like to see the results on public test data before doing any serious regularization & tuning.**\n",
    "\n",
    "Let's save the untuned LR model for later modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log_regression_model.sav']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'log_regression_model.sav'\n",
    "joblib.dump(lr, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = joblib.load(filename)\n",
    "#val_preds_lr = loaded_model.predict(tfidf_validation)\n",
    "#result = f1_score(y_val, val_preds_lr)\n",
    "#print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
